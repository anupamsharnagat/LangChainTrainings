{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0617eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Installation of required dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb49e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain) (1.1.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.12.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\rnd\\langchain\\langchaintrainings\\langchaintrainings\\lenv312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1742e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d0c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f98880",
   "metadata": {},
   "source": [
    "####Intracting with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb89e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Okay, let\\'s break down **LangChain**.\\n\\n### In Simple Terms\\n\\n**LangChain is a framework and set of tools designed to make it easier for developers to work with Large Language Models (LLMs).**\\n\\nThink of it like a construction kit for building applications powered by AI language models.\\n\\n### What it Does\\n\\nLangChain helps developers:\\n\\n1.  **Structure Interactions:** It provides tools to format user prompts and model responses effectively.\\n2.  **Chain Together Steps:** You can combine multiple LLM calls, API calls, data retrieval, and other operations into a single workflow (a \"Chain\").\\n3.  **Handle Complexity:** It offers ways to manage context, memory, handle large documents (chunking), and interact with external data sources.\\n4.  **Build Composable AI Apps:** It allows you to build modular components (like Agents or Chains) that can be combined to create sophisticated AI-powered applications.\\n\\n### Key Components (Think of these as building blocks)\\n\\n*   **Models:** Connects to various LLMs (like GPT-4, Llama, Gemini, etc.) and manages them.\\n*   **Prompts:** Helps create, format, and inject prompts with data retrieved from other sources.\\n*   **Chains:** Sequences of operations (LLM calls, function calls, data lookups, etc.) that run in a specific order.\\n*   **Agents:** More advanced components that can take actions based on goals, using tools (like Chains, APIs, web browsing) and memory.\\n*   **Memory:** Manages the context and history of a conversation or task for the LLM.\\n*   **Integrations:** Connects to external data sources (like Document AI, Search Engines, Databases) to provide information to the LLM.\\n\\n### Why Use LangChain?\\n\\n*   **Simplifies Complexity:** Working directly with raw LLM APIs and managing all the surrounding logic (context, data retrieval, structuring responses) can be complex. LangChain provides abstractions and tools to handle this complexity.\\n*   **Enables Advanced Use Cases:** It makes it easier to build applications that require multi-step reasoning, long-term memory, document processing (RAG - Retrieval-Augmented Generation), complex conversations, etc.\\n*   **Faster Development:** Provides pre-built components and patterns that can speed up the development of AI applications.\\n*   **Composability:** You can build small, reusable parts (like a specific Chain or Agent) and then combine them to create larger, more complex applications.\\n\\n### Analogy\\n\\nImagine you have a super powerful chef\\'s knife (the LLM). LangChain provides the cutting boards, measuring cups, specialized tools (spatulas, mixers), recipes (Chains), and ingredient storage (Memory) that help you use that knife effectively and safely to prepare complex meals (AI applications).\\n\\n### In Summary\\n\\nLangChain is a crucial toolkit in the rapidly growing field of AI development. It bridges the gap between powerful LLMs and the practical needs of building robust, real-world applications that leverage language model capabilities.' additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-12-14T12:11:40.3528478Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8240769400, 'load_duration': 64262400, 'prompt_eval_count': 7, 'prompt_eval_duration': 261359000, 'eval_count': 1126, 'eval_duration': 7817007100, 'logprobs': None, 'model_name': 'deepseek-r1:8b', 'model_provider': 'ollama'} id='lc_run--019b1cc5-e59a-7cc1-8afa-5d8b9abc6ab7-0' usage_metadata={'input_tokens': 7, 'output_tokens': 1126, 'total_tokens': 1133}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\", \n",
    "    temperature=0.5,\n",
    "    max_tokens=512\n",
    "    )\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1d5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1546a62",
   "metadata": {},
   "source": [
    "####Load env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065672da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith Tracing: AnlangchainBasics\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os   \n",
    "load_dotenv('./.env')\n",
    "print(\"Langsmith Tracing:\", os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4b5c7",
   "metadata": {},
   "source": [
    "#####PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a834e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is advantage of running llm in local environment?\n",
      "content=\"Running a Large Language Model (LLM) locally offers several distinct advantages, primarily related to control, privacy, performance, and customization. Here's a breakdown:\\n\\n1.  **Enhanced Privacy and Data Security:**\\n    *   **Data Stays On-Premises:** You never upload sensitive data (user inputs, documents, company information) to a third-party cloud server. The processing happens entirely on your own hardware, reducing the risk of data leakage or unauthorized access.\\n    *   **Compliance:** Easier to comply with strict data residency and security regulations (like GDPR, HIPAA, or internal corporate policies) since data doesn't leave your network.\\n\\n2.  **Full Control and Customization:**\\n    *   **Infrastructure Control:** You manage the hardware (servers, GPUs), operating systems, and network configurations. This allows for optimization tailored to the specific needs of running LLMs.\\n    *   **Model Customization:** You can fine-tune, retrain, or inject domain-specific knowledge directly into the model running locally. This is crucial for specialized applications (e.g., legal, medical, technical support).\\n    *   **Environment Control:** You have complete control over the software stack, libraries, and dependencies used to run and manage the LLM.\\n\\n3.  **Reduced Latency and Improved Performance:**\\n    *   **Faster Response Times:** Since data doesn't need to travel to a remote server and back, local inference is significantly faster, leading to a more responsive user experience, especially important for real-time applications.\\n    *   **No Network Bottlenecks:** Performance isn't affected by network speed, bandwidth limitations, or cloud provider availability issues.\\n\\n4.  **Offline Capabilities:**\\n    *   **No Internet Dependency:** The system can function even without an internet connection, which is vital for applications in remote locations, air-gapped environments, or areas with poor connectivity.\\n\\n5.  **Cost Efficiency (Potentially):**\\n    *   **Avoiding Cloud Costs:** While there's an upfront hardware investment, running the LLM locally can be more cost-effective for high-volume or continuous use compared to ongoing cloud API costs, especially as hardware costs for powerful GPUs continue to decrease.\\n    *   **Predictable Costs:** You pay for the hardware upfront and manage operational costs, avoiding the variable and sometimes unpredictable costs of cloud usage.\\n\\n6.  **Customization for Specific Use Cases:**\\n    *   **Tailored Hardware:** You can deploy hardware optimized for LLM inference (specialized AI accelerators alongside GPUs) which can be more efficient than general-purpose cloud instances.\\n    *   **Integration:** Easier to tightly integrate the LLM into existing on-premise systems and workflows without relying on cloud SDKs or APIs.\\n\\n**Important Considerations (Downsides):**\\n\\n*   **High Initial Hardware Costs:** Requires powerful computers (often with high-end GPUs) capable of running LLMs, which can be expensive.\\n*   **Complexity:** Requires expertise in hardware management, operating systems, networking, and often software engineering to deploy, manage, and scale the LLM.\\n*   **Maintenance Overhead:** You are responsible for updating the LLM, managing security patches, monitoring performance, and maintaining the entire infrastructure.\\n*   **Scalability Challenges:** Scaling up model size or inference capacity might require significant additional hardware investment, unlike the relative ease of scaling with cloud providers.\\n*   **Resource Consumption:** LLMs consume substantial amounts of CPU, GPU, memory, and storage resources.\\n\\n**In summary:** Running an LLM locally is ideal when you need maximum control, require strict data privacy, need offline functionality, want to heavily customize the model or infrastructure, or have specific use cases where cloud latency or connectivity isn't ideal. It requires significant technical expertise and investment in hardware, but offers unparalleled control and security.\" additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-12-14T13:00:35.0340963Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10118861000, 'load_duration': 1528775700, 'prompt_eval_count': 13, 'prompt_eval_duration': 20256100, 'eval_count': 1227, 'eval_duration': 8443632000, 'logprobs': None, 'model_name': 'deepseek-r1:8b', 'model_provider': 'ollama'} id='lc_run--019b1cf2-a5ef-79a0-b9b7-4f6d924fbed5-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1227, 'total_tokens': 1240}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"What is advantage of running llm in {context}?\")\n",
    "prompt = prompt_template.format(context=\"local environment\")\n",
    "print(prompt)\n",
    "content= llm.invoke(prompt)\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
